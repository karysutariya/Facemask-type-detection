{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e661ea-b07a-4ed1-8eac-fba2b5af1573",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c35df6f3-7145-4722-95da-e4ac612a0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5d22e-5673-4cf5-b145-de2db0c11492",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd62a6f-0fbd-45c4-bfe3-450d66adadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = '../Data/'\n",
    "CATEGORIES = ['Cloth mask','Mask worn incorrectly','N-95_Mask','No Face Mask','Surgical Mask']\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    i = 0\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            data.append(image)\n",
    "            labels.append(category)\n",
    "            #image = train_transforms(image)\n",
    "        except:\n",
    "            pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0a6e32-adf4-43e5-b090-f30932b72d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images = len(data)\n",
    "train_dataset_size = 1580\n",
    "test_dataset_size = total_images - train_dataset_size\n",
    "test_dataset_percentage = test_dataset_size/total_images\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=test_dataset_percentage, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b23d28c-98ab-4976-8f37-a7a77147c5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580\n",
      "519\n"
     ]
    }
   ],
   "source": [
    "# convert_tensor=transforms.ToTensor()\n",
    "# a = convert_tensor(data_train[3])\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd19d28-2fc4-4ec0-b138-70f17f1c2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet standards\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Train uses data augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "#        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(10),\n",
    "#        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "#        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))  \n",
    "])\n",
    "    \n",
    "# Validation does not use augmentation\n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "#        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44fe846e-2cb0-4f09-96aa-8ba754cb1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "labels_train = lb_make.fit_transform(labels_train)\n",
    "labels_test = lb_make.transform(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efde1b1b-10ef-406f-abd2-6eafbb3bedf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    try:\n",
    "        train_images.append(train_transforms(data_train[i]))\n",
    "        train_labels.append(labels_train[i])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "test_images = []\n",
    "test_labels = []\n",
    "for i in range(len(data_test)):\n",
    "    try:\n",
    "        test_images.append(test_transforms(data_test[i]))\n",
    "        test_labels.append(labels_test[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "72fdbf14-c667-4dbe-90db-0f566299ea49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506, 489, 1506, 489)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images), len(test_images), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a914b9-da08-46d7-bd23-0225718a8456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0be98fb6-f1e6-4c75-ae7c-ad3c6948dc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580, 519)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_train), len(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9a1f648-c91d-48c7-938c-75589b50ba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, ..., 2, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c88d62b0-3115-4307-acac-72a7d9634097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), 1506, 1506)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape, len(train_images), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a62bf0ca-8a2e-464a-8681-4c7a33ac7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_numpy = [t.numpy() for t in train_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "88cbfacf-8473-4853-a7af-8fe772e04d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_numpy = [t.numpy() for t in test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2d0f40cd-e97f-4ebb-b034-7962aad166cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 224, 224), 1506)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_numpy[0].shape, len(train_images_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "27ff1b70-8bae-4ff5-b4fa-e13c0797ab58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 224, 224), 489)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_numpy[0].shape, len(test_images_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "14399e98-b260-4ebe-a13f-6cd076666c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = {\n",
    "            'feature': torch.tensor([self.x[index]], dtype=torch.float32), \n",
    "            'label': torch.tensor([self.y[index]], dtype=torch.int16)}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4e83efff-5fe8-44c7-8c6b-ecfe9d565bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(x=train_images_numpy, y=train_labels)\n",
    "test_dataset = CreateDataset(x=test_images_numpy, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce0cc8b4-db19-4a32-99c1-e7b52c641725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_size = 1500\n",
    "# test_dataset_size = 601\n",
    "# train_data, test_data = random_split(torch_dataset, [train_dataset_size, test_dataset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f74ae354-14cf-45f8-8a63-e4f51e9420b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 100\n",
    "batch_size_test = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size_train,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                         batch_size=batch_size_test,\n",
    "                                         shuffle=False,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129986d-52a9-4ca3-b7b9-d44bc1f09297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db6faf20-505c-4a3b-94d4-9445a64613cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = torchvision.datasets.ImageFolder(root=train_dataset_path, transform=train_transforms)\n",
    "# test_dataset = torchvision.datasets.ImageFolder(root=test_dataset_path, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "47e801de-4fc0-4ac5-a5b1-a8fb8854a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transformed_images(dataset, batch_size):\n",
    "    #loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "    batch = next(iter(dataset))\n",
    "    images, labels = batch\n",
    "    \n",
    "    grid = trochvision.utils.make_grid(images, nrow = 3)\n",
    "    plt.figure(figsize=(11,11))\n",
    "    plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "    print('labels: ', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9be832b3-3bb0-4784-a670-7d9a2df389b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_transformed_images(dataset=train_loader, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fa32971e-0e0c-4cfc-92c6-1020e1d047f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.utils import make_grid\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def show_batch(dl):\n",
    "#     \"\"\"Plot images grid of single batch\"\"\"\n",
    "#     for images, labels in dl:\n",
    "#         fig,ax = plt.subplots(figsize = (16,12))\n",
    "#         ax.set_xticks([])\n",
    "#         ax.set_yticks([])\n",
    "#         ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "#         break\n",
    "        \n",
    "# show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e35d8b-692f-4cda-afb6-5428fe17522a",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc8cc3ad-49da-40ce-90e0-9fad2635cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "        \n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv_layer = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "#         nn.BatchNorm2d(32),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "#         nn.BatchNorm2d(32),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "        \n",
    "#         self.fc_layer = nn.Sequential(\n",
    "#         nn.Dropout(p=0.1),\n",
    "#         nn.Linear(28 * 28 * 128, 1000),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.Linear(1000, 512),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.Dropout(p=0.1),\n",
    "#         nn.Linear(512, 10)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # conv layers\n",
    "#         x = self.conv_layer(x)\n",
    "#         # flatten\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         # fc layer\n",
    "#         9\n",
    "#         https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\n",
    "#         6\n",
    "#         x = self.fc_layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bb97f45d-bcfc-44c0-8745-ce318ffa0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size = 3, padding = 1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(28 * 28 * 128, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 5)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1cafbcef-a1d1-4e79-a3fa-f02d343cee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f533f950-c3aa-4e3b-a851-1baf7c71f36a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26496/1639066748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            train_loss = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for i, data in enumerate(train_loader, start=1):\n",
    "                # forward\n",
    "                start_location = data['start_location']\n",
    "                file_cycle = data['file_cycle'].reshape(-1,1)\n",
    "                end_location = data['end_location']\n",
    "\n",
    "                outputs = self.model(start_location, file_cycle)\n",
    "                loss = self.criterion(outputs, end_location)\n",
    "\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                if epoch == num_epochs-1:\n",
    "                    pred_end_location_train.append(outputs)\n",
    "                    real_end_location_train.append(end_location)\n",
    "\n",
    "            train_loss_per_epoch.append(train_loss/len_train_loader)\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            self.model.eval()\n",
    "\n",
    "            for data in val_loader:\n",
    "                start_location = data['start_location']\n",
    "                file_cycle = data['file_cycle'].reshape(-1,1)\n",
    "                end_location = data['end_location'] \n",
    "\n",
    "                outputs = self.model(start_location, file_cycle)\n",
    "                loss = self.criterion(outputs, end_location)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                # if epoch == num_epochs-1:\n",
    "                #     pred_end_location_val.append(outputs)\n",
    "                #     real_end_location_val.append(end_location)\n",
    "\n",
    "            val_loss_per_epoch.append(valid_loss/len_val_loader)\n",
    "\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, Training Loss = {(train_loss/len_train_loader):.6f}\\\n",
    "            , Validation Loss: {(valid_loss/len_val_loader):.6f}')    \n",
    "\n",
    "        torch.save(self.model,'Saved/Models/'+self.flow_type+'/'+self.lag_type_train+'_'+str(self.num_seeds_train)+'_'+\\\n",
    "                   str(self.file_cycle_interval_train)+'_'+str(self.batch_size_train)+'_'+str(learning_rate)+'_Model.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9eb838de-240c-4767-8fde-1462c1109839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0cc4abb4-abe8-48f9-a5c1-486d9236977e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b26467b6-c19f-49bb-ab64-c4f49af73891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 3, 224, 224]) torch.Size([100, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26496/2599979623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\PytorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\PytorchEnv\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\.conda\\envs\\PytorchEnv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2846\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        images = data['feature']\n",
    "        labels = data['label']\n",
    "        print(images.shape, labels.shape)\n",
    "        images = images.reshape(-1,3,224,224)\n",
    "        #print(images.shape, labels.shape)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # Backprop and optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Train accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "            .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "            (correct / total) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35b61f-8801-41ee-b155-cba3c3dcd545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab3ece-e2f2-4b80-97dc-9c4461451319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e540d-59ab-490b-acc6-4b3393be0df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f989a-bdb4-45c1-8c85-eca746172f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"image_path\": [],\n",
    "    \"mask_status\": [],\n",
    "    \"where\": []\n",
    "}\n",
    "\n",
    "for where in os.listdir(path):\n",
    "    for status in os.listdir(path+\"/\"+where):\n",
    "        for image in glob.glob(path+where+\"/\"+status+\"/\"+\"*.png\"):\n",
    "            dataset[\"image_path\"].append(image)\n",
    "            dataset[\"mask_status\"].append(status)\n",
    "            dataset[\"where\"].append(where)\n",
    "            \n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a0c774d-f402-413d-8321-cb9daf046662",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_9344/3020880936.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Maryam\\AppData\\Local\\Temp/ipykernel_9344/3020880936.py\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(8 * 8 * 64, 1000),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(1000, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc layer\n",
    "        9\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\n",
    "        6\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d155a2b5-b662-4253-a859-a30c1139fd0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9344/3549313749.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtotal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0macc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        # Backprop and optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Train accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\\\n",
    "            (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d9620-066d-410e-a54f-145351006933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "path = '../Data/'\n",
    "cloth_mask_path = 'Cloth mask/'\n",
    "Mask_worn_incorrectly_path = 'Mask worn incorrectly/'\n",
    "N_95_Mask_path = 'N-95_Mask/'\n",
    "No_Face_Mask_path = 'No Face Mask/'\n",
    "Surgical_Mask_path = 'Surgical Mask/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
