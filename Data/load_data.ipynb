{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e661ea-b07a-4ed1-8eac-fba2b5af1573",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35df6f3-7145-4722-95da-e4ac612a0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5d22e-5673-4cf5-b145-de2db0c11492",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd62a6f-0fbd-45c4-bfe3-450d66adadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = '../Data/'\n",
    "CATEGORIES = ['Cloth mask','Mask worn incorrectly','N-95_Mask','No Face Mask','Surgical Mask']\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    i = 0\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            data.append(image)\n",
    "            labels.append(category)\n",
    "            #image = train_transforms(image)\n",
    "        except:\n",
    "            pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0a6e32-adf4-43e5-b090-f30932b72d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images = len(data)\n",
    "train_dataset_size = 1580\n",
    "test_dataset_size = total_images - train_dataset_size\n",
    "test_dataset_percentage = test_dataset_size/total_images\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=test_dataset_percentage, random_state=42)\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd19d28-2fc4-4ec0-b138-70f17f1c2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet standards\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Train uses data augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "#        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(10),\n",
    "#        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "#        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))  \n",
    "])\n",
    "    \n",
    "# Validation does not use augmentation\n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "#        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fe846e-2cb0-4f09-96aa-8ba754cb1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "labels_train = lb_make.fit_transform(labels_train)\n",
    "labels_val = lb_make.transform(labels_val)\n",
    "labels_test = lb_make.transform(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efde1b1b-10ef-406f-abd2-6eafbb3bedf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    try:\n",
    "        train_images.append(train_transforms(data_train[i]))\n",
    "        train_labels.append(labels_train[i])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "val_images = []\n",
    "val_labels = []\n",
    "for i in range(len(data_test)):\n",
    "    try:\n",
    "        val_images.append(test_transforms(data_val[i]))\n",
    "        val_labels.append(labels_val[i])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for i in range(len(data_test)):\n",
    "    try:\n",
    "        test_images.append(test_transforms(data_test[i]))\n",
    "        test_labels.append(labels_test[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62bf0ca-8a2e-464a-8681-4c7a33ac7256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maryam\\AppData\\Local\\Temp/ipykernel_22564/3818321563.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  train_images_tensor = torch.tensor(train_images_numpy)\n"
     ]
    }
   ],
   "source": [
    "train_images_numpy = [t.numpy() for t in train_images]\n",
    "val_images_numpy = [t.numpy() for t in val_images]\n",
    "test_images_numpy = [t.numpy() for t in test_images]\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels)\n",
    "val_labels_tensor = torch.tensor(val_labels)\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "train_images_tensor = torch.tensor(train_images_numpy)\n",
    "val_images_tensor = torch.tensor(val_images_numpy)\n",
    "test_images_tensor = torch.tensor(test_images_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3453bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_tensor = val_labels_tensor.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14399e98-b260-4ebe-a13f-6cd076666c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    def __init__(self, images, labels):\n",
    "        \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e83efff-5fe8-44c7-8c6b-ecfe9d565bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(images=train_images_tensor, labels=train_labels_tensor)\n",
    "val_dataset = CreateDataset(images=val_images_tensor, labels=val_labels_tensor)\n",
    "test_dataset = CreateDataset(images=test_images_tensor, labels=test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51cc6329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3,  ..., 2, 4, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f74ae354-14cf-45f8-8a63-e4f51e9420b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 100\n",
    "batch_size_test = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size_train,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=batch_size_train,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)                                           \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                         batch_size=batch_size_test,\n",
    "                                         shuffle=False,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47e801de-4fc0-4ac5-a5b1-a8fb8854a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transformed_images(dataset, batch_size):\n",
    "    #loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "    batch = next(iter(dataset))\n",
    "    images, labels = batch\n",
    "    \n",
    "    grid = trochvision.utils.make_grid(images, nrow = 3)\n",
    "    plt.figure(figsize=(11,11))\n",
    "    plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "    print('labels: ', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa32971e-0e0c-4cfc-92c6-1020e1d047f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.utils import make_grid\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def show_batch(dl):\n",
    "#     \"\"\"Plot images grid of single batch\"\"\"\n",
    "#     for images, labels in dl:\n",
    "#         fig,ax = plt.subplots(figsize = (16,12))\n",
    "#         ax.set_xticks([])\n",
    "#         ax.set_yticks([])\n",
    "#         ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "#         break\n",
    "        \n",
    "# show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e35d8b-692f-4cda-afb6-5428fe17522a",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb97f45d-bcfc-44c0-8745-ce318ffa0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size = 3, padding = 1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(28 * 28 * 128, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 5)\n",
    "        )\n",
    "\n",
    "        # self.final_layer = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        # x = self.final_layer(x)\n",
    "        # torch.argmax(x.reshape(-1))\n",
    "        return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cafbcef-a1d1-4e79-a3fa-f02d343cee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b26467b6-c19f-49bb-ab64-c4f49af73891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 41.2540, Accuracy: 32.00%\n",
      "Validation Accuracy of the model on the validation images: 40.0 %\n",
      "Epoch [1/100], Train Loss: 41.2540, Train Accuracy: 32.00, Validation Loss: 5.6374, Validation Accuracy: 40.00%\n",
      "Epoch [2/100], Loss: 2.0585, Accuracy: 39.77%\n",
      "Validation Accuracy of the model on the validation images: 43.0 %\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22564/796501608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m     print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}, Validation Loss: {:.4f}, Validation Accuracy: {:.2f}%'\n\u001b[0;32m     85\u001b[0m     .format(epoch + 1, num_epochs, loss_train_avg[epoch], acc_train_avg[epoch] * 100,\n\u001b[1;32m---> 86\u001b[1;33m     loss_val_avg[epoch], acc_val_avg[epoch] * 100))\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "total_step = len(train_loader)\n",
    "\n",
    "loss_list_train = []\n",
    "loss_list_val = []\n",
    "loss_train_avg = []\n",
    "loss_val_avg = []\n",
    "\n",
    "acc_list_train = []\n",
    "acc_list_val = []\n",
    "acc_train_avg = []\n",
    "acc_val_avg = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    loss_list_train = []\n",
    "    acc_list_train = []\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        images = images.reshape(-1,3,224,224)\n",
    "        #print(images.shape, labels.shape)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        #print(labels)\n",
    "        #print('outputs: ',outputs, outputs.shape)\n",
    "        #print(labels.shape)\n",
    "        #print(image)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list_train.append(loss.item())\n",
    "        \n",
    "        # Backprop and optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Train accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list_train.append(correct / total)\n",
    "        \n",
    "        # if (i + 1) % 100 == 0:\n",
    "        # print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "        # .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "        # (correct / total) * 100))\n",
    "\n",
    "\n",
    "\n",
    "    loss_train_avg.append(sum(loss_list_train)/len(train_loader))\n",
    "    acc_train_avg.append(sum(acc_list_train)/len(train_loader))\n",
    "    \n",
    "    # print('Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "    # .format(epoch + 1, num_epochs, loss_train_avg[epoch],\n",
    "    #  acc_train_avg[epoch] * 100))\n",
    "\n",
    "    loss_list_val = []\n",
    "    acc_list_val = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images,labels) in enumerate(val_loader):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list_val.append(loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            acc_list_val.append(correct / total)\n",
    "            # print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "            #     .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "            #     (correct / total) * 100))\n",
    "\n",
    "        loss_val_avg.append(sum(loss_list_val)/len(val_loader))\n",
    "        acc_val_avg.append(sum(acc_list_val)/len(val_loader))\n",
    "\n",
    "        # print('Validation Accuracy of the model on the validation images: {} %'\n",
    "        # .format((correct / total) * 100))\n",
    "\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}, Validation Loss: {:.4f}, Validation Accuracy: {:.2f}%'\n",
    "    .format(epoch + 1, num_epochs, loss_train_avg[epoch], acc_train_avg[epoch] * 100,\n",
    "    loss_val_avg[epoch], acc_val_avg[epoch] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe641435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maryam\\AppData\\Local\\Temp/ipykernel_25224/3959960696.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy of the model on the test images: 42.25 %\n"
     ]
    }
   ],
   "source": [
    "# Test:\n",
    "output_true = []\n",
    "output_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        output_true.append(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        output_pred.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing Accuracy of the model on the test images: {} %'\n",
    "        .format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e90eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28bf7dba3d0>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(loss_val_avg, 'b-1', markersize=12, markeredgewidth=3\n",
    "         , linewidth=2,label='val_loss')\n",
    "plt.plot(acc_val_avg, 'r--x', markersize=8, markeredgewidth=3\n",
    "         , linewidth=3,label='val_acc')\n",
    "plt.plot(loss_train_avg, 'c-1', markersize=12, markeredgewidth=3\n",
    "         , linewidth=2,label='training_loss')\n",
    "plt.plot(acc_train_avg, 'c-1', markersize=12, markeredgewidth=3\n",
    "         , linewidth=2,label='training_acc')\n",
    "plt.grid()\n",
    "plt.legend(loc='center right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68917acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(len(output_pred)*output_pred[0].shape[0])\n",
    "y_true = np.zeros(len(output_true)*output_true[0].shape[0])\n",
    "for i in range(len(output_pred)):\n",
    "    y_pred[i*output_pred[0].shape[0]:(i+1)*output_pred[0].shape[0]] = output_pred[i]\n",
    "    y_true[i*output_true[0].shape[0]:(i+1)*output_true[0].shape[0]] = output_true[i]\n",
    "    \n",
    "\n",
    "y_pred = y_pred.reshape(-1)\n",
    "y_true = y_true.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae418876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.21      0.29        67\n",
      "         1.0       0.41      0.23      0.29        87\n",
      "         2.0       0.33      0.85      0.48        81\n",
      "         3.0       0.76      0.61      0.68       100\n",
      "         4.0       0.49      0.26      0.34        65\n",
      "\n",
      "    accuracy                           0.45       400\n",
      "   macro avg       0.49      0.43      0.42       400\n",
      "weighted avg       0.50      0.45      0.43       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4eee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  7, 35,  5,  6],\n",
       "       [ 8, 20, 43,  9,  7],\n",
       "       [ 1,  3, 69,  3,  5],\n",
       "       [ 1, 13, 25, 61,  0],\n",
       "       [ 6,  6, 34,  2, 17]], dtype=int64)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e094a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b352cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52bc5f9a84c8b29c153e919e2e683c76286068b6632f704c8487631ce0b3b6d9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('PytorchEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
