{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e661ea-b07a-4ed1-8eac-fba2b5af1573",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35df6f3-7145-4722-95da-e4ac612a0bb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10984/280524546.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "## Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5d22e-5673-4cf5-b145-de2db0c11492",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd62a6f-0fbd-45c4-bfe3-450d66adadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = '../Data/'\n",
    "CATEGORIES = ['Cloth mask','Mask worn incorrectly','N-95_Mask','No Face Mask','Surgical Mask']\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    i = 0\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            data.append(image)\n",
    "            labels.append(category)\n",
    "            #image = train_transforms(image)\n",
    "        except:\n",
    "            pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0a6e32-adf4-43e5-b090-f30932b72d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images = len(data)\n",
    "train_dataset_size = 1580\n",
    "test_dataset_size = total_images - train_dataset_size\n",
    "test_dataset_percentage = test_dataset_size/total_images\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=test_dataset_percentage, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b23d28c-98ab-4976-8f37-a7a77147c5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580\n",
      "519\n"
     ]
    }
   ],
   "source": [
    "# convert_tensor=transforms.ToTensor()\n",
    "# a = convert_tensor(data_train[3])\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd19d28-2fc4-4ec0-b138-70f17f1c2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet standards\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Train uses data augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "#        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(10),\n",
    "#        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "#        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))  \n",
    "])\n",
    "    \n",
    "# Validation does not use augmentation\n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "#        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44fe846e-2cb0-4f09-96aa-8ba754cb1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "labels_train = lb_make.fit_transform(labels_train)\n",
    "labels_test = lb_make.transform(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efde1b1b-10ef-406f-abd2-6eafbb3bedf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    try:\n",
    "        train_images.append(train_transforms(data_train[i]))\n",
    "        train_labels.append(labels_train[i])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "test_images = []\n",
    "test_labels = []\n",
    "for i in range(len(data_test)):\n",
    "    try:\n",
    "        test_images.append(test_transforms(data_test[i]))\n",
    "        test_labels.append(labels_test[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "72fdbf14-c667-4dbe-90db-0f566299ea49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506, 489, 1506, 489)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images), len(test_images), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a914b9-da08-46d7-bd23-0225718a8456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0be98fb6-f1e6-4c75-ae7c-ad3c6948dc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580, 519)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_train), len(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9a1f648-c91d-48c7-938c-75589b50ba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, ..., 2, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c88d62b0-3115-4307-acac-72a7d9634097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), 1506, 1506)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape, len(train_images), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14399e98-b260-4ebe-a13f-6cd076666c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = {\n",
    "            'feature': torch.tensor([self.x[index]], dtype=torch.double), \n",
    "            'label': torch.tensor([self.y[index]], dtype=torch.int16)}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e83efff-5fe8-44c7-8c6b-ecfe9d565bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CreateDataset(x=train_images, y=train_labels)\n",
    "test_dataset = CreateDataset(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce0cc8b4-db19-4a32-99c1-e7b52c641725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_size = 1500\n",
    "# test_dataset_size = 601\n",
    "# train_data, test_data = random_split(torch_dataset, [train_dataset_size, test_dataset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f74ae354-14cf-45f8-8a63-e4f51e9420b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 100\n",
    "batch_size_test = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size_train,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                         batch_size=batch_size_test,\n",
    "                                         shuffle=False,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129986d-52a9-4ca3-b7b9-d44bc1f09297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db6faf20-505c-4a3b-94d4-9445a64613cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = torchvision.datasets.ImageFolder(root=train_dataset_path, transform=train_transforms)\n",
    "# test_dataset = torchvision.datasets.ImageFolder(root=test_dataset_path, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "47e801de-4fc0-4ac5-a5b1-a8fb8854a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transformed_images(dataset, batch_size):\n",
    "    #loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "    batch = next(iter(dataset))\n",
    "    images, labels = batch\n",
    "    \n",
    "    grid = trochvision.utils.make_grid(images, nrow = 3)\n",
    "    plt.figure(figsize=(11,11))\n",
    "    plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "    print('labels: ', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9be832b3-3bb0-4784-a670-7d9a2df389b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_transformed_images(dataset=train_loader, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fa32971e-0e0c-4cfc-92c6-1020e1d047f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.utils import make_grid\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def show_batch(dl):\n",
    "#     \"\"\"Plot images grid of single batch\"\"\"\n",
    "#     for images, labels in dl:\n",
    "#         fig,ax = plt.subplots(figsize = (16,12))\n",
    "#         ax.set_xticks([])\n",
    "#         ax.set_yticks([])\n",
    "#         ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "#         break\n",
    "        \n",
    "# show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e35d8b-692f-4cda-afb6-5428fe17522a",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc8cc3ad-49da-40ce-90e0-9fad2635cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "        \n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv_layer = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "#         nn.BatchNorm2d(32),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "#         nn.BatchNorm2d(32),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.LeakyReLU(inplace=True),\n",
    "#         nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "        \n",
    "#         self.fc_layer = nn.Sequential(\n",
    "#         nn.Dropout(p=0.1),\n",
    "#         nn.Linear(28 * 28 * 128, 1000),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.Linear(1000, 512),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#         nn.Dropout(p=0.1),\n",
    "#         nn.Linear(512, 10)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # conv layers\n",
    "#         x = self.conv_layer(x)\n",
    "#         # flatten\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         # fc layer\n",
    "#         9\n",
    "#         https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\n",
    "#         6\n",
    "#         x = self.fc_layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bb97f45d-bcfc-44c0-8745-ce318ffa0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size = 3, padding = 1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(28 * 28 * 128, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 5)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1cafbcef-a1d1-4e79-a3fa-f02d343cee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f533f950-c3aa-4e3b-a851-1baf7c71f36a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26496/1639066748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            train_loss = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for i, data in enumerate(train_loader, start=1):\n",
    "                # forward\n",
    "                start_location = data['start_location']\n",
    "                file_cycle = data['file_cycle'].reshape(-1,1)\n",
    "                end_location = data['end_location']\n",
    "\n",
    "                outputs = self.model(start_location, file_cycle)\n",
    "                loss = self.criterion(outputs, end_location)\n",
    "\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                if epoch == num_epochs-1:\n",
    "                    pred_end_location_train.append(outputs)\n",
    "                    real_end_location_train.append(end_location)\n",
    "\n",
    "            train_loss_per_epoch.append(train_loss/len_train_loader)\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            self.model.eval()\n",
    "\n",
    "            for data in val_loader:\n",
    "                start_location = data['start_location']\n",
    "                file_cycle = data['file_cycle'].reshape(-1,1)\n",
    "                end_location = data['end_location'] \n",
    "\n",
    "                outputs = self.model(start_location, file_cycle)\n",
    "                loss = self.criterion(outputs, end_location)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                # if epoch == num_epochs-1:\n",
    "                #     pred_end_location_val.append(outputs)\n",
    "                #     real_end_location_val.append(end_location)\n",
    "\n",
    "            val_loss_per_epoch.append(valid_loss/len_val_loader)\n",
    "\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, Training Loss = {(train_loss/len_train_loader):.6f}\\\n",
    "            , Validation Loss: {(valid_loss/len_val_loader):.6f}')    \n",
    "\n",
    "        torch.save(self.model,'Saved/Models/'+self.flow_type+'/'+self.lag_type_train+'_'+str(self.num_seeds_train)+'_'+\\\n",
    "                   str(self.file_cycle_interval_train)+'_'+str(self.batch_size_train)+'_'+str(learning_rate)+'_Model.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9eb838de-240c-4767-8fde-1462c1109839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         ...,\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "\n",
       "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         ...,\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "\n",
       "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         ...,\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b26467b6-c19f-49bb-ab64-c4f49af73891",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26496/3311942748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        image = data['feature']\n",
    "        label = data['label']\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # Backprop and optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Train accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "            .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "            (correct / total) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35b61f-8801-41ee-b155-cba3c3dcd545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab3ece-e2f2-4b80-97dc-9c4461451319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e540d-59ab-490b-acc6-4b3393be0df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f989a-bdb4-45c1-8c85-eca746172f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"image_path\": [],\n",
    "    \"mask_status\": [],\n",
    "    \"where\": []\n",
    "}\n",
    "\n",
    "for where in os.listdir(path):\n",
    "    for status in os.listdir(path+\"/\"+where):\n",
    "        for image in glob.glob(path+where+\"/\"+status+\"/\"+\"*.png\"):\n",
    "            dataset[\"image_path\"].append(image)\n",
    "            dataset[\"mask_status\"].append(status)\n",
    "            dataset[\"where\"].append(where)\n",
    "            \n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a0c774d-f402-413d-8321-cb9daf046662",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_9344/3020880936.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Maryam\\AppData\\Local\\Temp/ipykernel_9344/3020880936.py\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(8 * 8 * 64, 1000),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(1000, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc layer\n",
    "        9\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\n",
    "        6\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d155a2b5-b662-4253-a859-a30c1139fd0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9344/3549313749.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtotal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0macc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        # Backprop and optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Train accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\\\n",
    "            (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d9620-066d-410e-a54f-145351006933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "path = '../Data/'\n",
    "cloth_mask_path = 'Cloth mask/'\n",
    "Mask_worn_incorrectly_path = 'Mask worn incorrectly/'\n",
    "N_95_Mask_path = 'N-95_Mask/'\n",
    "No_Face_Mask_path = 'No Face Mask/'\n",
    "Surgical_Mask_path = 'Surgical Mask/'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02f47f39dd0ee8f21e529f9f6e1f62ff476995be84ab335999cbf3382766c67c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
